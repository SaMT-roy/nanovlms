lm_hidden_dim: int = 960
lm_inter_dim: int = 2560
lm_rms_eps: float = 1e-5
lm_re_base: int = 100000
lm_max_position_embeddings: int = 8192
lm_base_vocab_size: int = 49152
extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)
lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)
lm_n_heads: int = 15
lm_n_kv_heads: int = 5
lm_dropout: float = 0.0
lm_n_blocks: int = 32
lm_attn_scaling: float = 1.0
lm_max_length: int = 4096
lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights
lm_model_type = 'smollm2-360m'
lm_chat_template: str = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
